{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">Porto Seguro’s Safe Driver Prediction - Kaggle</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict if a driver will file an insurance claim next year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/safe-driving-730x432.jpeg' style='height:400px; border-radius: 5px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">Contents:</p>\n",
    "\n",
    "* [1. Introduction](#1)\n",
    "    * [1.1 Data Description](#1.1)\n",
    "    * [1.2 Libraries](#1.2)\n",
    "    * [1.3 Loading Dataset](#1.3)\n",
    "* [2. Preprocessing & Feature Engineering](#2)\n",
    "    * [2.1 SMOTE](#2.1)\n",
    "    * [2.2 Cross-validation](#2.2)\n",
    "    * [2.3 Feature Selection](#2.3)\n",
    "* [3. Models](#3)\n",
    "    * [3.1 Lightgbm](#3.1)\n",
    "    * [3.2 Neural Networks](#3.2)\n",
    "* [4. Evaluation](#4)\n",
    "* [5. Kaggle Submission](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">1- Introduction</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"Porto Seguro\" src=\"img/porto-seguro-logo-1-3.png\" alt=\"Porto Seguro\" style='height:80px; padding: 10px; padding-right: 15px;' align=\"center\"> \n",
    "\n",
    "[Porto Seguro](https://www.portoseguro.com.br/en/institutional) is one of the largest insurance companies in Brazil specialized in car and home insurance. Located in São Paulo, Porto Seguro has been one of the leading insurers in Brazil since its foundation in 1945.\n",
    "\n",
    "A key challenge faced by all major insurers is, when it comes to car insurance, how to address fairness towards good drivers and try not to penalize those who have a good driving history on account of a few bad drivers. Inaccuracies in car insurance claim predictions usually raise its cost for good drivers and reduce the price for bad ones.\n",
    "\n",
    "Porto Seguro has been applying Machine Learning for more than 20 years and intends to make car insurance more accessible to everyone. Thinking about that, the company created an online competition to help them explore new and more powerful ML methods.\n",
    "\n",
    "<img title=\"Porto Seguro\" src=\"img/Kaggle_logo.png\" alt=\"Porto Seguro\" style='height:80px; padding: 10px; padding-right: 15px' align=\"center\">\n",
    "\n",
    "[Kaggle](https://www.kaggle.com/) is an online community of data scientists and allows users to find and publish data sets, explore and build ML models, and enter competitions to solve data science challenges.\n",
    "\n",
    "In this [competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/), the challenge is build a model that predicts the probability that a car insurance policy holder will file a claim next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the train and test data:\n",
    "\n",
    "- Features that belong to similar groupings are tagged as such in the feature names (e.g., `ind`, `reg`, `car`, `calc`). \n",
    "- Feature names include the postfix `bin` to indicate binary features and `cat` to indicate categorical features.\n",
    "- Features __without__ these designations are either __continuous or ordinal__.\n",
    "- Values of `-1` indicate that the feature was missing from the observation. \n",
    "- The `target` columns signifies whether or not a claim was filed for that policy holder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ind` - individuals\n",
    "- `reg` - regions\n",
    "- `car` - cars\n",
    "- `calc` - calculated features\n",
    "\n",
    "- `_bin` - binary \n",
    "- `_cat` - categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocessing\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print('Python version:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U watermark\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Leandro Pessini\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.3\"></a>\n",
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle\n",
    "#kaggle_path = \"../input/porto-seguro-safe-driver-prediction/\"\n",
    "\n",
    "# Local\n",
    "local_path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(local_path + \"train.csv\").set_index('id')\n",
    "test_df = pd.read_csv(local_path + \"test.csv\").set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As per description, there are a few calculated features. In one of the discussions on Kaggle, it was highlighted that some kind of transformation was applied in order to generate these features. I will drop these features and apply the transformations using my best judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(train_df.filter(regex='_calc').columns, axis=1)\n",
    "test_df = test_df.drop(test_df.filter(regex='_calc').columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Dataset - Number of rows are',train_df.shape[0], 'and number of columns are ',train_df.shape[1])\n",
    "print('Test Dataset - Number of rows are',test_df.shape[0], 'and number of columns are ',test_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">2- Preprocessing & Feature Engineering</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target` variable 1 means that a claim was filed and 0 that it was not claimed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = train_df.target\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = sns.countplot(y,label=\"Count\")\n",
    "\n",
    "total_size = len(train_df)\n",
    "\n",
    "# Display the target value ratio at the top of the bar\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    width = patch.get_width()\n",
    "    left_coord = patch.get_x()\n",
    "    percent = height/total_size*100\n",
    "\n",
    "    ax.text(x=left_coord + width/2.0, \n",
    "            y=height + 3000,\n",
    "            s='{:1.1f}%'.format(percent),\n",
    "            ha='center')\n",
    "\n",
    "ax.set_title('Target Distribution');\n",
    "plt.savefig('./plots/target_distribution.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The target feature has a severe __imbalance distribution__ showing that only __3.6% filled a claim__ and 96.4% did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "Values of __`-1`__ indicate that the feature was missing from the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing = []\n",
    "\n",
    "for f in train_df.columns:\n",
    "    missings = train_df[train_df[f] == -1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings/train_df.shape[0]\n",
    "        \n",
    "        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
    "        \n",
    "print('\\nIn total, there are {} variables with missing values'.format(len(vars_with_missing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Only `ps_car_03_cat` and `ps_car_05_cat` have a large number (~ >= 50%) of missing values.\n",
    "- ps_car_03_cat has 411231 records (69.09%)\n",
    "- ps_car_05_cat has 266551 records (44.78%)\n",
    "\n",
    "There are several ways to __deal with missing data__ such as __Mean Imputation__, __Interpolation__ and __Extrapolation__. I will address this issue later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## Metadata\n",
    "\n",
    "To make data management easier, a meta-info about the variables is added to the DataFrame. It will help handling those variables later on the analysis, data viz and modeling.\n",
    "\n",
    "- __level__: categorical, numerical, binary\n",
    "- __dtype__: int, float, str\n",
    "\n",
    "We do not have information on which features are ordinal or not so a meta-info `numerical` will be added in order to apply __Normalization__ later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for f in train_df.columns:\n",
    "    \n",
    "    if 'bin' in f or f == 'target':\n",
    "        level = 'binary'\n",
    "    elif 'cat' in f:\n",
    "        level = 'categorical'\n",
    "    elif train_df[f].dtype == float:\n",
    "        level = 'numerical'\n",
    "    elif train_df[f].dtype == int:\n",
    "        level = 'numerical'\n",
    "    \n",
    "    # Defining the data type \n",
    "    dtype = train_df[f].dtype\n",
    "    \n",
    "    # Creating a Dict that contains all the metadata for the variable\n",
    "    f_dict = {\n",
    "        'varname': f,\n",
    "        'level': level,\n",
    "        'dtype': dtype\n",
    "    }\n",
    "    \n",
    "    data.append(f_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.DataFrame(data, columns=['varname', 'level', 'dtype'])\n",
    "meta.set_index('varname', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to extract all categorical variables that are not dropped\n",
    "meta[(meta.level == 'categorical')].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of variables per role and level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'count' : meta.groupby(['level'])['level'].size()}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = meta[(meta.level == 'numerical')].index\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "cont_corr = train_df[numerical_features].corr() # Correlation between continuous features\n",
    "sns.heatmap(cont_corr, annot=True, cmap='OrRd'); # Plot heatmap\n",
    "plt.savefig('./plots/heatmap.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a strong correlations between the variables:\n",
    "\n",
    "- ps_car_12 and ps_car_13 (0.67)\n",
    "- ps_reg_01 and ps_reg_03 (0.64)\n",
    "- ps_reg_02 and ps_reg_03 (0.52)\n",
    "\n",
    "Heatmap showed low number of correlated variables, we'll look at each of the highly correlated variables separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert -1 from training data copy to np.NaN\n",
    "train_copy = train_df.copy().replace(-1, np.NaN)\n",
    "train_copy = train_copy.dropna()\n",
    "s = train_copy.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: sampling was applied to speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps_car_12 x ps_car_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\n",
    "plt.savefig('./plots/ps_car_12xps_car_13.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps_reg_01 x ps_reg_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='ps_reg_01', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\n",
    "plt.savefig('./plots/ps_reg_01xps_reg_03.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps_reg_02 x ps_reg_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\n",
    "plt.savefig('./plots/ps_reg_02xps_reg_03.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of correlated variables is rather low, dimensionality reduction will not be applied and the model will do the heavy-lifting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Distribution__ of binary data and the __corresponding__ values of __target__ variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore')\n",
    "var = [col for col in train_copy.columns if '_bin' in col]\n",
    "i = 0\n",
    "\n",
    "s_bin = train_copy.sample(frac=0.1)\n",
    "t1 = s_bin.loc[s_bin['target'] != 0]\n",
    "t0 = s_bin.loc[s_bin['target'] == 0]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(15,20))\n",
    "\n",
    "for feature in var:\n",
    "    i += 1\n",
    "    plt.subplot(4,3,i)\n",
    "    sns.kdeplot(t1[feature], bw=0.5, label=\"target = 1\")\n",
    "    sns.kdeplot(t0[feature], bw=0.5, label=\"target = 0\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('Density plot', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.savefig('./plots/binary-features.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the categorical variables are already numerical, there is no need to apply LabelEncoding.\n",
    "\n",
    "__Reference__:\n",
    ">Raschka, S., & Mirjalili, V. (2019). Python Machine Learning. Zaltbommel, Netherlands: Van Haren Publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "numerical_features = meta[(meta.level == 'numerical')].index\n",
    "features_n = numerical_features.to_list()\n",
    "training_normalized = train_df.copy()\n",
    "\n",
    "features = training_normalized[features_n]\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "\n",
    "training_normalized[features_n] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_normalized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'is_unbalance': True, # because training data is extremely unbalanced\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'dart',\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 250,\n",
    "        'n_jobs': 2, # number of parallel threads\n",
    "        'importance_type': 'gain'\n",
    "    }\n",
    "features_classifier = lgb.LGBMClassifier()\n",
    "features_classifier.set_params(**lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__StratifiedKFold__: This method should be used in situations when we have a very unbalanced class.\n",
    "\n",
    "- __'objective': 'binary'__ is because it is a classification problem\n",
    "\n",
    "- __'learning_rate'__ Step size shrinkage used in update to prevents overfitting. After each boosting step, we can diresctly get the weights of new features. Learning rate shrinks the feature weights to make the boosting process more conservative.\n",
    "\n",
    "- __'boosting_type'__ gbdt suffers from over-specialization, which means trees added at later iterations tend to impact the prediction of only a few instances and make a negligible contribution towards the remaining instances. Adding dropout makes it more difficult for the trees at later iterations to specialize on those few samples and hence improves the performance. For this reason I am using __DART (Dropouts meet Multiple Additive Regression Trees)__ as boosting type.\n",
    "\n",
    ">Rashmi, K. V., & Gilad-Bachrach, R. (2015). DART: Dropouts meet Multiple Additive Regression Trees. ArXiv.\n",
    "\n",
    "- __'n_estimators__ Number of boosted trees to fit.\n",
    "- __'n_jobs': 2__ for speed improvement, set this to the number of real CPU cores, not the number of threads.\n",
    "- __'importance_type': 'gain'__ result contains total gains of splits which use the feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = training_data.target\n",
    "X = training_data.drop(['target'], inplace=False, axis=1)\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
    "predicts = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print('-'*40)\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    features_classifier.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=250, eval_metric=[\"auc\", \"binary\"])\n",
    "    predicts.append(features_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting features importance\n",
    "\n",
    "feature_imp = pd.DataFrame(sorted(zip(features_classifier.feature_importances_,X.columns)), columns=['Value','Feature'])\n",
    "scaler_ft = MinMaxScaler()\n",
    "feature_imp['Value'] = scaler_ft.fit_transform(feature_imp['Value'].values.reshape(-1,1))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features Importance by avg over folds')\n",
    "plt.savefig('./plots/lgbm_importances.png', dpi=fig.dpi)\n",
    "locs, labels = plt.xticks()\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Feature Importance, let's concat the train and test data in order to perform transformation on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1) # Remove target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping less important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_features = ['ps_car_08_cat', 'ps_ind_13_bin','ps_ind_10_bin', \n",
    "                 'ps_ind_11_bin','ps_ind_12_bin','ps_car_10_cat', \n",
    "                 'ps_ind_14','ps_car_05_cat','ps_ind_18_bin', \n",
    "                 'ps_car_02_cat','ps_ind_08_bin','ps_car_12']\n",
    "\n",
    "\n",
    "all_data_remaining = all_data.drop(drop_features, axis=1)\n",
    "\n",
    "print('Number of features before selection: {}'.format(all_data.shape[1]))\n",
    "print('Number of features after selection: {}'.format(all_data_remaining.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_feat_sel = all_data_remaining.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The study of missing data was formalized by Donald Rubin with the concept of missing mechanism in which missing-data indicators are random variables and assigned a distribution. Missing data mechanism describes the underlying mechanism that generates missing data.\n",
    "\n",
    "It is important to consider missing data mechanism when deciding how to deal with missing data. Because this is unknown, I will consider the missing data as part of the dataset (new category for example) and just create a new feature adding the total number of missing data.\n",
    "\n",
    ">Rubin, D. B. (1975). INFERENCE AND MISSING DATA. ETS Research Bulletin Series, 1975(1), i–19. https://doi.org/10.1002/j.2333-8504.1975.tb01053.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = all_data_feat_sel.columns.tolist()\n",
    "num_features = [c for c in feature_names if '_cat' not in c]\n",
    "all_data_feat_sel['missing'] = (all_data_feat_sel==-1).sum(axis=1).astype(float)\n",
    "num_features.append('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## Feature scaling using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_n = [col for col in all_data_feat_sel.columns if ('_cat' not in col and '_bin' not in col)]\n",
    "all_data_n = all_data_feat_sel.copy()\n",
    "\n",
    "features = all_data_n[features_n]\n",
    "\n",
    "# using default \n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "\n",
    "all_data_n[features_n] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "## One-hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_enc = all_data_n.copy()\n",
    "categoricals_features = [col for col in all_data_feat_sel.columns if '_cat' in col]\n",
    "\n",
    "print('Before dummification we have {} variables in train'.format(all_data_enc.shape[1]))\n",
    "all_data_enc = pd.get_dummies(all_data_enc, columns=categoricals_features, drop_first=True)\n",
    "print('After dummification we have {} variables in train'.format(all_data_enc.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = train_df.shape[0] # Number of train data \n",
    "final_data = all_data_enc.copy()\n",
    "\n",
    "# Divide train data and test data\n",
    "X = np.asarray(final_data[:num_train])\n",
    "X_test = np.asarray(final_data[num_train:])\n",
    "\n",
    "y = np.asarray(train_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE (undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">3- Models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Gini coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gini](./img/gini_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for why use this instead of the commonly used AUC, the only reason I can think of is that a random prediction will yield a Gini score of 0 as opposed to the AUC which will be 0.5.\n",
    "\n",
    "Besides that using the gini coefficient sets the performance of a random classifier to a score of 0... the normalization \"improves\" the other end of the scale and makes that the score of a perfect classifier is equal to 1 rather than a maximum achievable AUC<1. The improvement being only relative depending on whether you think a more intuitive scale is good or not. Although beyond this easier interpretation you might argue that it (the normalization) also improves generalization and comparison of different data-sets.\n",
    "\n",
    "__Reference:__\n",
    "\n",
    "> Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171–186. https://doi.org/10.1023/a:1010920819831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "\n",
    "    # sort rows on prediction column\n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n",
    "    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n",
    "\n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) * 1. / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) * 1. / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1 / n_samples, 1, n_samples)\n",
    "\n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "\n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred * 1. / G_true\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', Gini(labels, preds), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'is_unbalance': True,\n",
    "        'metric': 'auc',\n",
    "        'feature_fraction': 0.7,\n",
    "        'max_bin': 255,\n",
    "        'n_jobs': 2,\n",
    "        'min_data_in_leaf': 1500,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 30\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __On smaller data sets__ If you have a huge data set, you can afford to slice it into three representative parts — train, test, final evaluation. Unfortunately, you don’t always have this luxury; your data set might not be large enough to slice into three representative parts. This is where cross validation comes in. K-fold cross validation prevents overfitting to your test data without further reducing the size of your training data set.\n",
    "- __To get a less biased (read: optimistic) evaluation of your model__ K-fold cross validation reports on the performance of a model on several (k) samples from your training set. This gives you insight into the distribution of accuracy you are likely to see for a model.\n",
    "- __When you have enough computing resources/time__ K-fold cross validation is more computationally expensive than slicing your data into three parts. It re-fits the model and tests it k-times, for each iteration, as opposed to one time. Thus, It is much more valuable with small data sets where this runtime/computational cost is not significant, and the extra partition would reduce the training data set significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-dimensional array of probabilities for predicting validation data target values\n",
    "val_preds_lgb = np.zeros(X.shape[0]) \n",
    "# One-dimensional array of probabilities for predicting test data target values\n",
    "test_preds_lgb = np.zeros(X_test.shape[0])\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "# Create Stratified K Fold CV\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # The phrase that separates each fold.\n",
    "    print('#'*40, f'Fold {n_fold+1} out of {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx] # Train data\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] # Valid data\n",
    "\n",
    "    # Create lgbm dataset\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # lgbm train dataset\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # lgbm valid dataset\n",
    "\n",
    "    lgb_model = lgb.train(params=lgb_params,\n",
    "                          train_set=dtrain,\n",
    "                          num_boost_round=1500,\n",
    "                          valid_sets=[dvalid, dtrain],\n",
    "                          feval=gini_lgb, # Evaluation metrics for validation\n",
    "                          early_stopping_rounds=200, \n",
    "                          verbose_eval=200, \n",
    "                          evals_result=evals_result)\n",
    "    \n",
    "    # The number of boosting iterations when the model performs best \n",
    "    best_iter = lgb_model.best_iteration\n",
    "    \n",
    "    # Predict probabilities using test data\n",
    "    test_preds_lgb += lgb_model.predict(X_test, \n",
    "                                    num_iteration=best_iter)/folds.n_splits\n",
    "    # prediction for model performance evaluation\n",
    "    val_preds_lgb[valid_idx] += lgb_model.predict(X_valid, num_iteration=best_iter)\n",
    "    \n",
    "    # Normalized Gini coefficient for prediction probabilities\n",
    "    gini_score = Gini(y_valid, val_preds_lgb[valid_idx])\n",
    "    print(f'Fold {n_fold+1} gini score: {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LightGBM model to file in the current working directory\n",
    "\n",
    "joblib_file = \"LightGBM_Model.pkl\"  \n",
    "joblib.dump(lgb_model, joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Search for Hyper-Parameter Optimization\n",
    "- Grid search\n",
    "- Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "param_dist = {'learning_rate': [0.1, 0.01, 0.001],\n",
    "              'max_bin': [200, 300, 500],\n",
    "              'min_data_in_leaf': [500, 1000, 1500],\n",
    "              'feature_fraction': [0.3, 0.5, 0.7],\n",
    "              'n_estimators': [400, 700, 1000, 2000, 3000],\n",
    "              'num_leaves': [10, 20, 30, 40]\n",
    "              }\n",
    "\n",
    "n_iter_search = 20\n",
    "randomsearh_lgb = lgb.LGBMClassifier()\n",
    "random_search = RandomizedSearchCV(randomsearh_lgb, \n",
    "                                   param_distributions = param_dist, \n",
    "                                   n_iter = n_iter_search,\n",
    "                                   return_train_score=True)\n",
    "\n",
    "\n",
    "start = time()\n",
    "random_search.fit(final_data[:num_train], train_df['target'])\n",
    "print(\"RandomizedSearchCV ran in %.2f seconds to %d candidates to best parameters.\" \n",
    "      % ((time() - start), n_iter_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'is_unbalance': True,\n",
    "        'metric': 'auc',\n",
    "        'feature_fraction': 0.7,\n",
    "        'max_bin': 255,\n",
    "        'n_jobs': 2,\n",
    "        'min_data_in_leaf': 1500,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 30\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">4- Evaluation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('LightGBM Gini Score:', Gini(y, val_preds_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(evals_result, metric='gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(evals_result, metric='auc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "joblib_LightGBM_model = joblib.load(joblib_file)\n",
    "joblib_LightGBM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_LightGBM_model.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_LightGBM_model.get_split_value_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Reloaded Joblib Model to \n",
    "# Calculate the accuracy score and predict target values\n",
    "\n",
    "# Calculate the Score \n",
    "score = joblib_LightGBM_model.score(X, y)  \n",
    "# Print the Score\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))  \n",
    "\n",
    "# Predict the Labels using the reloaded Model\n",
    "Ypredict = joblib_LightGBM_model.predict(X_test)  \n",
    "\n",
    "Ypredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">5- Kaggle Submission</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(local_path + 'sample_submission.csv', index_col='id')\n",
    "submission['target'] = test_preds_lgb\n",
    "submission.to_csv('./kaggle-submission/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./ignore-folder/score/scores.png' style='height:800px; border-radius: 5px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./ignore-folder/score/private_leaderboard.png' style='height:600px; border-radius: 5px;'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
