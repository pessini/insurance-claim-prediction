{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">Porto Seguro’s Safe Driver Prediction - Kaggle</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict if a driver will file an insurance claim next year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/safe-driving-730x432.jpeg' style='height:400px; border-radius: 5px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">Contents:</p>\n",
    "\n",
    "* [1. Introduction](#1)\n",
    "    * [1.1 Data Description](#1.1)\n",
    "    * [1.2 Libraries](#1.2)\n",
    "    * [1.3 Loading Dataset](#1.3)\n",
    "* [2. Preprocessing & Feature Engineering](#2)\n",
    "    * [2.1 SMOTE](#2.1)\n",
    "    * [2.2 Cross-validation](#2.2)\n",
    "    * [2.3 Feature Selection](#2.3)\n",
    "* [3. Models](#3)\n",
    "    * [3.1 Lightgbm](#3.1)\n",
    "    * [3.2 Neural Networks](#3.2)\n",
    "* [4. Evaluation](#4)\n",
    "* [5. Kaggle Submission](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">1- Introduction</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"Porto Seguro\" src=\"img/porto-seguro-logo-1-3.png\" alt=\"Porto Seguro\" style='height:80px; padding: 10px; padding-right: 15px;' align=\"center\"> \n",
    "\n",
    "[Porto Seguro](https://www.portoseguro.com.br/en/institutional) is one of the largest insurance companies in Brazil specialized in car and home insurance. Located in São Paulo, Porto Seguro has been one of the leading insurers in Brazil since its foundation in 1945.\n",
    "\n",
    "A key challenge faced by all major insurers is, when it comes to car insurance, how to address fairness towards good drivers and try not to penalize those who have a good driving history on account of a few bad drivers. Inaccuracies in car insurance claim predictions usually raise its cost for good drivers and reduce the price for bad ones.\n",
    "\n",
    "Porto Seguro has been applying Machine Learning for more than 20 years and intends to make car insurance more accessible to everyone. Thinking about that, the company created an online competition to help them explore new and more powerful ML methods.\n",
    "\n",
    "<img title=\"Porto Seguro\" src=\"img/Kaggle_logo.png\" alt=\"Porto Seguro\" style='height:80px; padding: 10px; padding-right: 15px' align=\"center\">\n",
    "\n",
    "[Kaggle](https://www.kaggle.com/) is an online community of data scientists and allows users to find and publish data sets, explore and build ML models, and enter competitions to solve data science challenges.\n",
    "\n",
    "In this [competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/), the challenge is build a model that predicts the probability that a car insurance policy holder will file a claim next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the train and test data:\n",
    "\n",
    "- Features that belong to similar groupings are tagged as such in the feature names (e.g., `ind`, `reg`, `car`, `calc`). \n",
    "- Feature names include the postfix `bin` to indicate binary features and `cat` to indicate categorical features.\n",
    "- Features __without__ these designations are either __continuous or ordinal__.\n",
    "- Values of `-1` indicate that the feature was missing from the observation. \n",
    "- The `target` columns signifies whether or not a claim was filed for that policy holder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ind` - individuals\n",
    "- `reg` - regions\n",
    "- `car` - cars\n",
    "- `calc` - calculated features\n",
    "\n",
    "- `_bin` - binary \n",
    "- `_cat` - categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import random\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "\n",
    "# Preprocessing\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import joblib\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Hyperparameter optimization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print('Python version:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U watermark\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Leandro Pessini\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.3\"></a>\n",
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle\n",
    "#path = \"../input/porto-seguro-safe-driver-prediction/\"\n",
    "\n",
    "# Local\n",
    "path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(path + \"train.csv\").set_index('id')\n",
    "test_df = pd.read_csv(path + \"test.csv\").set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As per description, there are a few calculated features. In one of the discussions on Kaggle, it was highlighted that some kind of transformation was applied in order to generate these features. I will drop these features and apply the transformations using my best judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(train_df.filter(regex='_calc').columns, axis=1)\n",
    "test_df = test_df.drop(test_df.filter(regex='_calc').columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Dataset - Number of rows are',train_df.shape[0], 'and number of columns are ',train_df.shape[1])\n",
    "print('Test Dataset - Number of rows are',test_df.shape[0], 'and number of columns are ',test_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">2- Preprocessing & Feature Engineering</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target` variable 1 means that a claim was filed and 0 that it was not claimed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = train_df.target\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = sns.countplot(y,label=\"Count\")\n",
    "\n",
    "total_size = len(train_df)\n",
    "\n",
    "# Display the target value ratio at the top of the bar\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    width = patch.get_width()\n",
    "    left_coord = patch.get_x()\n",
    "    percent = height/total_size*100\n",
    "\n",
    "    ax.text(x=left_coord + width/2.0, \n",
    "            y=height + 3000,\n",
    "            s='{:1.1f}%'.format(percent),\n",
    "            ha='center')\n",
    "\n",
    "ax.set_title('Target Distribution');\n",
    "plt.savefig('./plots/target_distribution.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The target feature has a severe __imbalance distribution__ showing that only __3.6% filled a claim__ and 96.4% did not.\n",
    "\n",
    "This will be handle by the algorithm through a hyperparameter `is_unbalance = True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "Values of __`-1`__ indicate that the feature was missing from the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing = []\n",
    "\n",
    "for f in train_df.columns:\n",
    "    missings = train_df[train_df[f] == -1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings/train_df.shape[0]\n",
    "        \n",
    "        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
    "        \n",
    "print('\\nIn total, there are {} variables with missing values'.format(len(vars_with_missing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Only `ps_car_03_cat` and `ps_car_05_cat` have a large number (~ >= 50%) of missing values.\n",
    "- ps_car_03_cat has 411231 records (69.09%)\n",
    "- ps_car_05_cat has 266551 records (44.78%)\n",
    "\n",
    "There are several ways to __deal with missing data__ such as __Mean Imputation__, __Interpolation__ and __Extrapolation__. I will address this issue later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## Metadata\n",
    "\n",
    "To make data management easier, a meta-info about the variables is added to the DataFrame. It will help handling those variables later on the analysis, data viz and modeling.\n",
    "\n",
    "- __level__: categorical, numerical, binary\n",
    "- __dtype__: int, float, str\n",
    "\n",
    "We do not have information on which features are ordinal or not so a meta-info `numerical` will be added in order to apply __Normalization__ later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for f in train_df.columns:\n",
    "    \n",
    "    if 'bin' in f or f == 'target':\n",
    "        level = 'binary'\n",
    "    elif 'cat' in f:\n",
    "        level = 'categorical'\n",
    "    elif train_df[f].dtype == float:\n",
    "        level = 'numerical'\n",
    "    elif train_df[f].dtype == int:\n",
    "        level = 'numerical'\n",
    "    \n",
    "    # Defining the data type \n",
    "    dtype = train_df[f].dtype\n",
    "    \n",
    "    # Creating a Dict that contains all the metadata for the variable\n",
    "    f_dict = {\n",
    "        'varname': f,\n",
    "        'level': level,\n",
    "        'dtype': dtype\n",
    "    }\n",
    "    \n",
    "    data.append(f_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.DataFrame(data, columns=['varname', 'level', 'dtype'])\n",
    "meta.set_index('varname', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to extract all categorical variables that are not dropped\n",
    "meta[(meta.level == 'categorical')].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of variables per role and level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'count' : meta.groupby(['level'])['level'].size()}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = meta[(meta.level == 'numerical')].index\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "cont_corr = train_df[numerical_features].corr() # Correlation between continuous features\n",
    "sns.heatmap(cont_corr, annot=True, cmap='OrRd'); # Plot heatmap\n",
    "plt.savefig('./plots/heatmap.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a strong correlations between the variables:\n",
    "\n",
    "- ps_car_12 and ps_car_13 (0.67)\n",
    "- ps_reg_01 and ps_reg_03 (0.64)\n",
    "- ps_reg_02 and ps_reg_03 (0.52)\n",
    "\n",
    "Heatmap showed low number of correlated variables, we'll look at each of the highly correlated variables separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert -1 from training data copy to np.NaN\n",
    "train_copy = train_df.copy().replace(-1, np.NaN)\n",
    "train_copy = train_copy.dropna()\n",
    "s = train_copy.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: sampling was applied to speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps_car_12 x ps_car_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\n",
    "plt.savefig('./plots/ps_car_12xps_car_13.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps_reg_01 x ps_reg_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='ps_reg_01', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\n",
    "plt.savefig('./plots/ps_reg_01xps_reg_03.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps_reg_02 x ps_reg_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\n",
    "plt.savefig('./plots/ps_reg_02xps_reg_03.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of correlated variables is rather low, dimensionality reduction will not be applied and the model will do the heavy-lifting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Distribution__ of binary data and the __corresponding__ values of __target__ variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore')\n",
    "var = [col for col in train_copy.columns if '_bin' in col]\n",
    "i = 0\n",
    "\n",
    "s_bin = train_copy.sample(frac=0.1)\n",
    "t1 = s_bin.loc[s_bin['target'] != 0]\n",
    "t0 = s_bin.loc[s_bin['target'] == 0]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(15,20))\n",
    "\n",
    "for feature in var:\n",
    "    i += 1\n",
    "    plt.subplot(4,3,i)\n",
    "    sns.kdeplot(t1[feature], bw=0.5, label=\"target = 1\")\n",
    "    sns.kdeplot(t0[feature], bw=0.5, label=\"target = 0\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('Density plot', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "#plt.savefig('./plots/binary-features.png', dpi=fig.dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the categorical variables are already numerical, there is no need to apply LabelEncoding.\n",
    "\n",
    "__Reference__:\n",
    ">Raschka, S., & Mirjalili, V. (2019). Python Machine Learning. Zaltbommel, Netherlands: Van Haren Publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "numerical_features = meta[(meta.level == 'numerical')].index\n",
    "features_n = numerical_features.to_list()\n",
    "training_normalized = train_df.copy()\n",
    "\n",
    "features = training_normalized[features_n]\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "\n",
    "training_normalized[features_n] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_normalized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_params_f = {\n",
    "        'is_unbalance': True, # because training data is extremely unbalanced\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'dart',\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 250,\n",
    "        'n_jobs': 2, # number of parallel threads\n",
    "        'importance_type': 'gain'\n",
    "    }\n",
    "features_classifier = lgb.LGBMClassifier()\n",
    "features_classifier.set_params(**lgb_params_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__StratifiedKFold__: This method should be used in situations when we have a very unbalanced class.\n",
    "\n",
    "- __'objective': 'binary'__ is because it is a classification problem\n",
    "\n",
    "- __'learning_rate'__ Step size shrinkage used in update to prevents overfitting. After each boosting step, we can diresctly get the weights of new features. Learning rate shrinks the feature weights to make the boosting process more conservative.\n",
    "\n",
    "- __'boosting_type'__ gbdt suffers from over-specialization, which means trees added at later iterations tend to impact the prediction of only a few instances and make a negligible contribution towards the remaining instances. Adding dropout makes it more difficult for the trees at later iterations to specialize on those few samples and hence improves the performance. For this reason I am using __DART (Dropouts meet Multiple Additive Regression Trees)__ as boosting type.\n",
    "\n",
    ">Rashmi, K. V., & Gilad-Bachrach, R. (2015). DART: Dropouts meet Multiple Additive Regression Trees. ArXiv.\n",
    "\n",
    "- __'n_estimators__ Number of boosted trees to fit.\n",
    "- __'n_jobs': 2__ for speed improvement, set this to the number of real CPU cores, not the number of threads.\n",
    "- __'importance_type': 'gain'__ result contains total gains of splits which use the feature.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y = training_data.target\n",
    "X = training_data.drop(['target'], inplace=False, axis=1)\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
    "predicts = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print('-'*40)\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    features_classifier.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=250, eval_metric=[\"auc\", \"binary\"])\n",
    "    predicts.append(features_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Plotting features importance\n",
    "\n",
    "feature_imp = pd.DataFrame(sorted(zip(features_classifier.feature_importances_,X.columns)), columns=['Value','Feature'])\n",
    "scaler_ft = MinMaxScaler()\n",
    "feature_imp['Value'] = scaler_ft.fit_transform(feature_imp['Value'].values.reshape(-1,1))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features Importance by avg over folds')\n",
    "plt.savefig('./plots/lgbm_importances.png', dpi=fig.dpi)\n",
    "locs, labels = plt.xticks()\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Feature Importance, let's concat the train and test data in order to perform transformation on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1) # Remove target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping less important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_features = ['ps_car_08_cat', 'ps_ind_13_bin','ps_ind_12_bin', \n",
    "                 'ps_car_10_cat','ps_ind_10_bin','ps_ind_11_bin', \n",
    "                 'ps_ind_14','ps_car_02_cat','ps_car_05_cat', \n",
    "                 'ps_ind_08_bin','ps_car_09_cat','ps_ind_02_cat']\n",
    "\n",
    "\n",
    "all_data_remaining = all_data.drop(drop_features, axis=1)\n",
    "\n",
    "print('Number of features before selection: {}'.format(all_data.shape[1]))\n",
    "print('Number of features after selection: {}'.format(all_data_remaining.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_feat_sel = all_data_remaining.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The study of missing data was formalized by Donald Rubin with the concept of missing mechanism in which missing-data indicators are random variables and assigned a distribution. Missing data mechanism describes the underlying mechanism that generates missing data.\n",
    "\n",
    "It is important to consider missing data mechanism when deciding how to deal with missing data. Because this is unknown, I will consider the missing data as part of the dataset (new category for example) and just create a new feature adding the total number of missing data.\n",
    "\n",
    ">Rubin, D. B. (1975). INFERENCE AND MISSING DATA. ETS Research Bulletin Series, 1975(1), i–19. https://doi.org/10.1002/j.2333-8504.1975.tb01053.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = all_data_feat_sel.columns.tolist()\n",
    "num_features = [c for c in feature_names if '_cat' not in c]\n",
    "all_data_feat_sel['missing'] = (all_data_feat_sel==-1).sum(axis=1).astype(float)\n",
    "num_features.append('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## Feature scaling using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_n = [col for col in all_data_feat_sel.columns if ('_cat' not in col and '_bin' not in col)]\n",
    "all_data_n = all_data_feat_sel.copy()\n",
    "\n",
    "features = all_data_n[features_n]\n",
    "\n",
    "# using default \n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "\n",
    "all_data_n[features_n] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "## One-hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_enc = all_data_n.copy()\n",
    "categoricals_features = [col for col in all_data_feat_sel.columns if '_cat' in col]\n",
    "\n",
    "print('Before dummification we have {} variables in train'.format(all_data_enc.shape[1]))\n",
    "all_data_enc = pd.get_dummies(all_data_enc, columns=categoricals_features, drop_first=True)\n",
    "print('After dummification we have {} variables in train'.format(all_data_enc.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = train_df.shape[0] # Number of train data \n",
    "final_data = all_data_enc.copy()\n",
    "\n",
    "# Divide train data and test data\n",
    "X = np.asarray(final_data[:num_train])\n",
    "X_test = np.asarray(final_data[num_train:])\n",
    "\n",
    "y = np.asarray(train_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">3- Model</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini coefficient - Kaggle Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gini](./img/lorenz-curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini index or Gini coefficient is a statistical measure of distribution which was developed by the Italian statistician Corrado Gini in 1912. It is used as a gauge of economic inequality, measuring income distribution among a population.\n",
    "\n",
    "The Gini coefficient is equal to the area below the line of perfect equality (0.5 by definition) minus the area below the Lorenz curve, divided by the area below the line of perfect equality. In other words, it is double the area between the Lorenz curve and the line of perfect equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reference__:\n",
    ">https://theblog.github.io/post/gini-coefficient-intuitive-explanation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "\n",
    "    # sort rows on prediction column\n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n",
    "    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n",
    "\n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) * 1. / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) * 1. / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1 / n_samples, 1, n_samples)\n",
    "\n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "\n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred * 1. / G_true\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', Gini(labels, preds), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'is_unbalance': True, # As we have a severe imbalanced class\n",
    "        'metric': 'auc',\n",
    "        'n_jobs': 2,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 30\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__\n",
    "- __'is_unbalance': True__\n",
    "\n",
    "Sets the weights of the dominated label to 1, and the weights of the dominant labels to the ratio of count of dominant/dominated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your data set is not be large enough to slice into three representative parts, cross validation can help with that. K-fold cross validation prevents overfitting to your test data without further reducing the size of your training data set.\n",
    "\n",
    "K-fold cross validation reports on the performance of a model on several (k) samples from your training set. This provides a less biased evaluation of the model. However, K-fold cross validation is more computationally expensive than slicing your data into three parts. It re-fits the model and tests it k-times, for each iteration, as opposed to one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# One-dimensional array of probabilities for predicting validation data target values\n",
    "val_preds_lgb = np.zeros(X.shape[0]) \n",
    "# One-dimensional array of probabilities for predicting test data target values\n",
    "test_preds_lgb = np.zeros(X_test.shape[0])\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "# Create Stratified K Fold CV\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # The phrase that separates each fold.\n",
    "    print('#'*40, f'Fold {n_fold+1} out of {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx] # Train data\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] # Valid data\n",
    "\n",
    "    # Create lgbm dataset\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # lgbm train dataset\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # lgbm valid dataset\n",
    "\n",
    "    lgb_model = lgb.train(params=lgb_params,\n",
    "                          train_set=dtrain,\n",
    "                          num_boost_round=1500,\n",
    "                          valid_sets=[dvalid, dtrain],\n",
    "                          feval=gini_lgb, # Evaluation metrics for validation\n",
    "                          early_stopping_rounds=200, \n",
    "                          verbose_eval=200, \n",
    "                          evals_result=evals_result)\n",
    "    \n",
    "    # The number of boosting iterations when the model performs best \n",
    "    best_iter = lgb_model.best_iteration\n",
    "    \n",
    "    # Predict probabilities using test data\n",
    "    test_preds_lgb += lgb_model.predict(X_test, \n",
    "                                    num_iteration=best_iter)/folds.n_splits\n",
    "    # prediction for model performance evaluation\n",
    "    val_preds_lgb[valid_idx] += lgb_model.predict(X_valid, num_iteration=best_iter)\n",
    "    \n",
    "    # Normalized Gini coefficient for prediction probabilities\n",
    "    gini_score = Gini(y_valid, val_preds_lgb[valid_idx])\n",
    "    print(f'Fold {n_fold+1} gini score: {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LightGBM model to file in the current working directory\n",
    "lgb_model.save_model('./files/model-gini/LightGBM_Model_gini.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LightGBM model to file in the current working directory\n",
    "\n",
    "joblib_file = \"./files/model-gini/LightGBM_Model_gini.pkl\"  \n",
    "joblib.dump(lgb_model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files/model-gini/model_gini_evals_result.json', 'w') as fp:\n",
    "    json.dump(evals_result, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./files/model-gini/validation_preds_lgb.csv', val_preds_lgb, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./files/model-gini/test_preds_lgb.csv', test_preds_lgb, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search for Hyper-Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search for Hyper-Parameter Optimization\n",
    "\n",
    ">https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Randomized Search CV\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "param_dist = {'learning_rate': [0.1, 0.01, 0.001],\n",
    "              'max_bin': [500, 1000, 2000],\n",
    "              'min_data_in_leaf': [500, 1000, 1500],\n",
    "              'n_estimators': [500, 1500, 2000],\n",
    "              'num_leaves': [10, 30, 50, 100],\n",
    "              'bagging_freq': [1, 3, 5],\n",
    "              'bagging_fraction': [0.25, 0.5, 0.75]\n",
    "              }\n",
    "\n",
    "n_iter_search = 10\n",
    "randomsearh_lgb = lgb.LGBMClassifier()\n",
    "random_search = RandomizedSearchCV(randomsearh_lgb, \n",
    "                                   param_distributions = param_dist, \n",
    "                                   n_iter = n_iter_search,\n",
    "                                   return_train_score=True)\n",
    "start = time()\n",
    "random_search.fit(final_data[:num_train], train_df['target'])\n",
    "print(\"RandomizedSearchCV: %.2f seconds for %d candidates to best parameters.\" \n",
    "      % ((time() - start), n_iter_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__NOTE__: Due to computing resources limitations I did not perform RandomizedSearchCV locally\n",
    "\n",
    "__Parameters Tuning__ was performed on Kaggle and yield the output below:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RandomizedSearchCV](./img/randomSearchCV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`random_search.best_params_`\n",
    "\n",
    "![Best-Params](./img/best-params.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Tunned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_tuned = {\n",
    "                'objective': 'binary',\n",
    "                'is_unbalance': True, # As we have a severe imbalanced class\n",
    "                'metric': 'auc',\n",
    "                'n_jobs': 2,\n",
    "                'num_leaves': 50,\n",
    "                # num_boost_round = 2500 - will be added in train()\n",
    "                'max_bin': 2000,\n",
    "                'learning_rate': 0.001,\n",
    "                'min_data_in_leaf': 200,\n",
    "                'feature_fraction': 0.7,\n",
    "                'bagging_freq': 1,\n",
    "                'bagging_fraction': 0.5\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# One-dimensional array of probabilities for predicting validation data target values\n",
    "val_preds_lgb_tuned = np.zeros(X.shape[0]) \n",
    "# One-dimensional array of probabilities for predicting test data target values\n",
    "test_preds_lgb_tuned = np.zeros(X_test.shape[0])\n",
    "\n",
    "evals_result_tuned = {}\n",
    "\n",
    "# Create Stratified K Fold CV\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # The phrase that separates each fold.\n",
    "    print('#'*40, f'Fold {n_fold+1} out of {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx] # Train data\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] # Valid data\n",
    "\n",
    "    # Create lgbm dataset\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # lgbm train dataset\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # lgbm valid dataset\n",
    "\n",
    "    lgb_model_tuned = lgb.train(params=lgb_params_tuned,\n",
    "                          train_set=dtrain,\n",
    "                          num_boost_round=2500, # n_estimators\n",
    "                          valid_sets=[dvalid, dtrain],\n",
    "                          feval=gini_lgb, # Evaluation metrics for validation\n",
    "                          early_stopping_rounds=200, \n",
    "                          verbose_eval=500, \n",
    "                          evals_result=evals_result_tuned)\n",
    "    \n",
    "    # The number of boosting iterations when the model performs best \n",
    "    best_iter_tuned = lgb_model_tuned.best_iteration\n",
    "    \n",
    "    # Predict probabilities using test data\n",
    "    test_preds_lgb_tuned += lgb_model_tuned.predict(X_test, \n",
    "                                    num_iteration=best_iter_tuned)/folds.n_splits\n",
    "    # prediction for model performance evaluation\n",
    "    val_preds_lgb_tuned[valid_idx] += lgb_model_tuned.predict(X_valid, num_iteration=best_iter_tuned)\n",
    "    \n",
    "    # Normalized Gini coefficient for prediction probabilities\n",
    "    gini_score = Gini(y_valid, val_preds_lgb_tuned[valid_idx])\n",
    "    print(f'Fold {n_fold+1} gini score: {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-899e27b44ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_preds_lgb_tuned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_precision_recall_vs_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y, val_preds_lgb_tuned)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LightGBM model to file in the current working directory\n",
    "lgb_model_tuned.save_model('./files/model-gini-tuned/LightGBM_Model_tuned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LightGBM model to file in the current working directory\n",
    "joblib_filelgb_model_tuned = \"./files/model-gini-tuned/LightGBM_Model_tuned.pkl\"  \n",
    "joblib.dump(lgb_model_tuned, joblib_filelgb_model_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files/model-gini-tuned/model_tuned_evals_result.json', 'w') as fp:\n",
    "    json.dump(evals_result_tuned, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_lgb_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./files/model-gini-tuned/validation_preds_lgb_tuned.csv', val_preds_lgb_tuned, fmt = '%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./files/model-gini-tuned/test_preds_lgb_tuned.csv', test_preds_lgb_tuned, fmt = '%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kflfds = np.fromstring(loadtxt('./files/model-gini-tuned/validation_preds_lgb_tuned.csv', dtype=float))\n",
    "# print the array\n",
    "kflfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y, val_preds_lgb_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM with Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_local = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'dart',\n",
    "        'metric': 'average_precision',\n",
    "        'n_jobs': 2,\n",
    "        'num_leaves': 70,\n",
    "        'max_bin': 2000,\n",
    "        'learning_rate': 0.001,\n",
    "        'min_data_in_leaf': 200,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.5,\n",
    "        'lambda_l1': 1,\n",
    "        'lambda_l2': 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# One-dimensional array of probabilities for predicting validation data target values\n",
    "val_preds_lgb_local = np.zeros(X.shape[0]) \n",
    "# One-dimensional array of probabilities for predicting test data target values\n",
    "test_preds_lgb_local = np.zeros(X_test.shape[0])\n",
    "\n",
    "evals_result_local = {}\n",
    "\n",
    "# Create Stratified K Fold CV\n",
    "folds_local = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds_local.split(X, y)):\n",
    "    # The phrase that separates each fold.\n",
    "    print('#'*40, f'Fold {n_fold+1} out of {folds_local.n_splits}', '#'*40)\n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx] # Train data\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] # Valid data\n",
    "\n",
    "    # Create lgbm dataset\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # lgbm train dataset\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # lgbm valid dataset\n",
    "\n",
    "    lgb_model_local = lgb.train(params=lgb_params_local,\n",
    "                                train_set=dtrain,\n",
    "                                num_boost_round=3500,\n",
    "                                valid_sets=[dvalid, dtrain],\n",
    "                                early_stopping_rounds=150, \n",
    "                                verbose_eval=1000, \n",
    "                                evals_result=evals_result_local)\n",
    "    \n",
    "    # The number of boosting iterations when the model performs best \n",
    "    best_iter_local = lgb_model_local.best_iteration\n",
    "    \n",
    "    # Predict probabilities using test data\n",
    "    test_preds_lgb_local += lgb_model_local.predict(X_test, \n",
    "                                    num_iteration=best_iter_local)/folds_local.n_splits\n",
    "    # prediction for model performance evaluation\n",
    "    val_preds_lgb_local[valid_idx] += lgb_model_local.predict(X_valid, num_iteration=best_iter_local)\n",
    "    \n",
    "    # Normalized Gini coefficient for prediction probabilities\n",
    "    gini_score = Gini(y_valid, val_preds_lgb_local[valid_idx])\n",
    "    print(f'Fold {n_fold+1} gini score: {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LightGBM model to file in the current working directory\n",
    "lgb_model_local.save_model('./files/model-local/LightGBM_Model_local.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LightGBM model to file in the current working directory\n",
    "joblib_filelgb_model_local = \"./files/model-local/LightGBM_Model_local.pkl\"  \n",
    "joblib.dump(lgb_model_local, joblib_filelgb_model_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files/model-local/model_local_evals_result.json', 'w') as fp:\n",
    "    json.dump(evals_result_local, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./files/model-local/validation_preds_lgb_local.csv', val_preds_lgb_local, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./files/model-local/test_preds_lgb_local.csv', test_preds_lgb_local, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">4- Evaluation</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic metrics are designed specifically to quantify the uncertainty in a classifier’s predictions. Algorithms such as Logistic Regression, Random Forest and Gradient Boosting give probability outputs. Probability outputs can be converted to class output by creating a threshold probability.\n",
    "\n",
    "\n",
    "- Are you predicting probabilities?\n",
    "    - Do you need class labels?\n",
    "        - Is the positive class more important?\n",
    "            Use Precision-Recall AUC\n",
    "        - Are both classes important?\n",
    "            Use ROC AUC\n",
    "        - Do you need probabilities?\n",
    "            Use Brier Score and Brier Skill Score\n",
    "- Are you predicting class labels?\n",
    "    - Is the positive class more important?\n",
    "        - Are False Negatives and False Positives Equally Important?\n",
    "            Use F1-Measure\n",
    "        - Are False Negatives More Important?\n",
    "            Use F2-Measure\n",
    "        - Are False Positives More Important?\n",
    "            Use F0.5-Measure\n",
    "    - Are both classes important?\n",
    "        - Do you have < 80%-90% Examples for the Majority Class? \n",
    "            Use Accuracy\n",
    "        - Do you have > 80%-90% Examples for the Majority Class? \n",
    "            Use G-Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n",
    "\n",
    ">https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved module (erase later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_file = \"LightGBM_Model_default.pkl\"\n",
    "# Load from file\n",
    "joblib_LightGBM_model = joblib.load(joblib_file)\n",
    "joblib_LightGBM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test - test_csv sem target\n",
    "# X, y train\n",
    "\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_predictions_lightgbm = np.zeros(X.shape[0]) \n",
    "test_predictions_lightgbm = np.zeros(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_lightgbm = joblib_LightGBM_model.predict(X_test, \n",
    "                                                          num_iteration=joblib_LightGBM_model.best_iteration)\n",
    "validation_predictions_lightgbm = joblib_LightGBM_model.predict(X_valid, \n",
    "                                                                num_iteration=joblib_LightGBM_model.best_iteration)\n",
    "\n",
    "# Normalized Gini coefficient for prediction probabilities\n",
    "gini_score = Gini(y_valid, validation_predictions_lightgbm)\n",
    "print(f'gini score: {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_model = lgb.Booster(model_file=joblib_LightGBM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(first_model, metric='auc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-Recall AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LightGBM Gini Score:', Gini(y, val_preds_lgb_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(evals_result_local)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_valid = lgb_model.best_score[\"valid_0\"]\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y, val_preds_lgb)\n",
    "\n",
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(y, val_preds_lgb)\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = plot_precision_recall_curve(lgb, X, y)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LightGBM Gini Score:', Gini(y, val_preds_lgb_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(evals_result_tuned, metric='gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(evals_result_tuned, metric='auc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini - Kaggle Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('LightGBM Gini Score:', Gini(y, val_preds_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(evals_result, metric='gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_metric(evals_result, metric='auc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">5- Kaggle Submission</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path + 'sample_submission.csv', index_col='id')\n",
    "submission['target'] = test_preds_lgb\n",
    "submission.to_csv('./kaggle-submission/LightGBM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path + 'sample_submission.csv', index_col='id')\n",
    "submission['target'] = test_preds_lgb_tuned\n",
    "submission.to_csv('./kaggle-submission/LightGBM_tuned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./ignore-folder/score/scores.png' style='height:800px; border-radius: 5px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./ignore-folder/score/private_leaderboard.png' style='height:600px; border-radius: 5px;'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
