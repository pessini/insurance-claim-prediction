{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">Porto Seguro’s Safe Driver Prediction - Kaggle</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict if a driver will file an insurance claim next year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/safe-driving-730x432.jpeg' style='height:400px; border-radius: 5px;'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">Contents:</p>\n",
    "\n",
    "* [1. Introduction](#1)\n",
    "    * [1.1 Data Description](#1.1)\n",
    "    * [1.2 Libraries](#1.2)\n",
    "    * [1.3 Loading Dataset](#1.3)\n",
    "* [2. Feature Engineering](#2)\n",
    "    * [2.1 SMOTE](#2.1)\n",
    "    * [2.2 Cross-validation](#2.2)\n",
    "    * [2.3 Feature Selection](#2.3)\n",
    "* [3. Models](#3)\n",
    "    * [3.1 Lightgbm](#3.1)\n",
    "    * [3.2 Neural Networks](#3.2)\n",
    "* [4. Evaluation](#4)\n",
    "* [5. Kaggle Submission](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">1- Introduction</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"Porto Seguro\" src=\"img/porto-seguro-logo-1-3.png\" alt=\"Porto Seguro\" style='height:80px; padding: 10px; padding-right: 15px;' align=\"center\"> \n",
    "\n",
    "[Porto Seguro](https://www.portoseguro.com.br/en/institutional) is one of the largest insurance companies in Brazil specialized in car and home insurance. Located in São Paulo, Porto Seguro has been one of the leading insurers in Brazil since its foundation in 1945.\n",
    "\n",
    "A key challenge faced by all major insurers is, when it comes to car insurance, how to address fairness towards good drivers and try not to penalize those who have a good driving history on account of a few bad drivers. Inaccuracies in car insurance claim predictions usually raise its cost for good drivers and reduce the price for bad ones.\n",
    "\n",
    "Porto Seguro has been applying Machine Learning for more than 20 years and intends to make car insurance more accessible to everyone. Thinking about that, the company created an online competition to help them explore new and more powerful ML methods.\n",
    "\n",
    "<img title=\"Porto Seguro\" src=\"img/Kaggle_logo.png\" alt=\"Porto Seguro\" style='height:80px; padding: 10px; padding-right: 15px' align=\"center\">\n",
    "\n",
    "[Kaggle](https://www.kaggle.com/) is an online community of data scientists and allows users to find and publish data sets, explore and build ML models, and enter competitions to solve data science challenges.\n",
    "\n",
    "In this [competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/), the challenge is build a model that predicts the probability that a car insurance policy holder will file a claim next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the train and test data:\n",
    "\n",
    "- Features that belong to similar groupings are tagged as such in the feature names (e.g., `ind`, `reg`, `car`, `calc`). \n",
    "- Feature names include the postfix `bin` to indicate binary features and `cat` to indicate categorical features.\n",
    "- Features __without__ these designations are either __continuous or ordinal__.\n",
    "- Values of `-1` indicate that the feature was missing from the observation. \n",
    "- The `target` columns signifies whether or not a claim was filed for that policy holder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ind` - individuals\n",
    "- `reg` - regions\n",
    "- `car` - cars\n",
    "- `calc` - calculated features\n",
    "\n",
    "- `_bin` - binary \n",
    "- `_cat` - categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python version:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Leandro Pessini\n",
      "\n",
      "matplotlib: 3.3.2\n",
      "seaborn   : 0.11.1\n",
      "plotly    : 4.14.3\n",
      "pandas    : 1.1.3\n",
      "numpy     : 1.19.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U watermark\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Leandro Pessini\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.3\"></a>\n",
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle\n",
    "#kaggle_path = \"../input/porto-seguro-safe-driver-prediction/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "local_path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training dataset and converting missing data value to NaN\n",
    "#train_df = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv', low_memory=False, na_values= '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(local_path + \"train.csv\").set_index('id')\n",
    "test_df = pd.read_csv(local_path + \"test.csv\").set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset - Number of rows are 595212 and number of columns are  58\n",
      "Test Dataset - Number of rows are 892816 and number of columns are  57\n"
     ]
    }
   ],
   "source": [
    "print('Train Dataset - Number of rows are',train_df.shape[0], 'and number of columns are ',train_df.shape[1])\n",
    "print('Test Dataset - Number of rows are',test_df.shape[0], 'and number of columns are ',test_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">2- Preprocessing & Feature Engineering</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "## Metadata\n",
    "\n",
    "To make data management easier, a meta-info about the variables is added to the DataFrame. It will help handling those variables later on the analysis, data viz and modeling.\n",
    "\n",
    "- __role__: input, ID, target\n",
    "- __level__: categorical, numerical, binary\n",
    "- __keep__: True or False\n",
    "- __dtype__: int, float, str\n",
    "\n",
    "We do not have information on which features are ordinal or not so a meta-info __numerical__ will be added in order to apply __Normalization__ later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for f in train_df.columns:\n",
    "    # Defining the role\n",
    "    if f == 'target':\n",
    "        role = 'target'\n",
    "    else:\n",
    "        role = 'input'\n",
    "         \n",
    "    # Defining the level\n",
    "    if 'bin' in f or f == 'target':\n",
    "        level = 'binary'\n",
    "        train_df[f] = train_df[f].astype('int8')\n",
    "    elif 'cat' in f:\n",
    "        level = 'categorical'\n",
    "        train_df[f] = train_df[f].astype('int32')\n",
    "    elif train_df[f].dtype == float:\n",
    "        level = 'numerical'\n",
    "        train_df[f] = train_df[f].astype('float32')\n",
    "    elif train_df[f].dtype == int:\n",
    "        level = 'numerical'\n",
    "        train_df[f] = train_df[f].astype('int32')\n",
    "        \n",
    "    # Initialize keep to True for all variables\n",
    "    keep = True\n",
    "    \n",
    "    # Defining the data type \n",
    "    dtype = train_df[f].dtype\n",
    "    \n",
    "    # Creating a Dict that contains all the metadata for the variable\n",
    "    f_dict = {\n",
    "        'varname': f,\n",
    "        'role': role,\n",
    "        'level': level,\n",
    "        'keep': keep,\n",
    "        'dtype': dtype\n",
    "    }\n",
    "    \n",
    "    data.append(f_dict)\n",
    "    \n",
    "meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\n",
    "meta.set_index('varname', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat',\n",
       "       'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat',\n",
       "       'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat',\n",
       "       'ps_car_10_cat', 'ps_car_11_cat'],\n",
       "      dtype='object', name='varname')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to extract all categorical variables that are not dropped\n",
    "meta[(meta.level == 'categorical') & (meta.keep)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of variables per role and level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>role</th>\n",
       "      <th>level</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>input</td>\n",
       "      <td>binary</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>input</td>\n",
       "      <td>categorical</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>input</td>\n",
       "      <td>numerical</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>target</td>\n",
       "      <td>binary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     role        level  count\n",
       "0   input       binary     17\n",
       "1   input  categorical     14\n",
       "2   input    numerical     26\n",
       "3  target       binary      1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.900378</td>\n",
       "      <td>4.423318</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>7.299922</td>\n",
       "      <td>0.610991</td>\n",
       "      <td>0.439184</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>2.346072</td>\n",
       "      <td>0.379945</td>\n",
       "      <td>0.813265</td>\n",
       "      <td>...</td>\n",
       "      <td>1.885886</td>\n",
       "      <td>7.689445</td>\n",
       "      <td>3.005823</td>\n",
       "      <td>9.225904</td>\n",
       "      <td>2.339034</td>\n",
       "      <td>8.433590</td>\n",
       "      <td>5.441382</td>\n",
       "      <td>1.441918</td>\n",
       "      <td>2.872288</td>\n",
       "      <td>7.539026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.983789</td>\n",
       "      <td>2.699902</td>\n",
       "      <td>0.127545</td>\n",
       "      <td>3.546042</td>\n",
       "      <td>0.287643</td>\n",
       "      <td>0.404264</td>\n",
       "      <td>0.793506</td>\n",
       "      <td>0.832548</td>\n",
       "      <td>0.058327</td>\n",
       "      <td>0.224588</td>\n",
       "      <td>...</td>\n",
       "      <td>1.134927</td>\n",
       "      <td>1.334312</td>\n",
       "      <td>1.414564</td>\n",
       "      <td>1.459672</td>\n",
       "      <td>1.246949</td>\n",
       "      <td>2.904597</td>\n",
       "      <td>2.332871</td>\n",
       "      <td>1.202963</td>\n",
       "      <td>1.694887</td>\n",
       "      <td>2.746652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.250619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.670867</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.720677</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.765811</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.906190</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>4.037945</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>3.720626</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ps_ind_01      ps_ind_03      ps_ind_14      ps_ind_15  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        1.900378       4.423318       0.012451       7.299922   \n",
       "std         1.983789       2.699902       0.127545       3.546042   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       2.000000       0.000000       5.000000   \n",
       "50%         1.000000       4.000000       0.000000       7.000000   \n",
       "75%         3.000000       6.000000       0.000000      10.000000   \n",
       "max         7.000000      11.000000       4.000000      13.000000   \n",
       "\n",
       "           ps_reg_01      ps_reg_02      ps_reg_03      ps_car_11  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.610991       0.439184       0.551102       2.346072   \n",
       "std         0.287643       0.404264       0.793506       0.832548   \n",
       "min         0.000000       0.000000      -1.000000      -1.000000   \n",
       "25%         0.400000       0.200000       0.525000       2.000000   \n",
       "50%         0.700000       0.300000       0.720677       3.000000   \n",
       "75%         0.900000       0.600000       1.000000       3.000000   \n",
       "max         0.900000       1.800000       4.037945       3.000000   \n",
       "\n",
       "           ps_car_12      ps_car_13  ...     ps_calc_05     ps_calc_06  \\\n",
       "count  595212.000000  595212.000000  ...  595212.000000  595212.000000   \n",
       "mean        0.379945       0.813265  ...       1.885886       7.689445   \n",
       "std         0.058327       0.224588  ...       1.134927       1.334312   \n",
       "min        -1.000000       0.250619  ...       0.000000       0.000000   \n",
       "25%         0.316228       0.670867  ...       1.000000       7.000000   \n",
       "50%         0.374166       0.765811  ...       2.000000       8.000000   \n",
       "75%         0.400000       0.906190  ...       3.000000       9.000000   \n",
       "max         1.264911       3.720626  ...       6.000000      10.000000   \n",
       "\n",
       "          ps_calc_07     ps_calc_08     ps_calc_09     ps_calc_10  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        3.005823       9.225904       2.339034       8.433590   \n",
       "std         1.414564       1.459672       1.246949       2.904597   \n",
       "min         0.000000       2.000000       0.000000       0.000000   \n",
       "25%         2.000000       8.000000       1.000000       6.000000   \n",
       "50%         3.000000       9.000000       2.000000       8.000000   \n",
       "75%         4.000000      10.000000       3.000000      10.000000   \n",
       "max         9.000000      12.000000       7.000000      25.000000   \n",
       "\n",
       "          ps_calc_11     ps_calc_12     ps_calc_13     ps_calc_14  \n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000  \n",
       "mean        5.441382       1.441918       2.872288       7.539026  \n",
       "std         2.332871       1.202963       1.694887       2.746652  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         4.000000       1.000000       2.000000       6.000000  \n",
       "50%         5.000000       1.000000       3.000000       7.000000  \n",
       "75%         7.000000       2.000000       4.000000       9.000000  \n",
       "max        19.000000      10.000000      13.000000      23.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features = meta[(meta.level == 'numerical') & (meta.keep)].index\n",
    "train_df[numerical_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "## Feature scaling using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_n = numerical_features.to_list()\n",
    "train_normalized = train_df.copy()\n",
    "\n",
    "features = train_normalized[features_n]\n",
    "\n",
    "scaler = MinMaxScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "\n",
    "train_normalized[features_n] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "## Categorical encoding using One-Hot-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode with Keras\n",
    "\n",
    "You may have a sequence that is already integer encoded.\n",
    "\n",
    "You could work with the integers directly, after some scaling. Alternately, you can one hot encode the integers directly. This is important to consider if the integers do not have a real ordinal relationship and are really just placeholders for labels.\n",
    "\n",
    "The Keras library offers a function called to_categorical() that you can use to one hot encode integer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "categorical_features = meta[(meta.level == 'categorical') & (meta.keep)].index # Categorical features\n",
    "name_features_c = categorical_features.to_list()\n",
    "\n",
    "train_transformed = train_normalized.copy()\n",
    "\n",
    "features_c = train_transformed[name_features_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "\n",
    "encoded = to_categorical(features_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat with original data\n",
    "train_transformed = pd.concat([train_transformed, ohe_df], axis=1).drop(features_c, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Other Options__: If you are doing hyperparameter tuning with [GridSearch](https://scikit-learn.org/stable/modules/grid_search.html) it's recommended to use [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) and [FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) with [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) or directly [make_column_transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed[numerical_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed[categorical_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_encoder = OneHotEncoder()\n",
    "jobs_encoder.fit(data['Profession'].unique().reshape(1, -1))\n",
    "transformed = jobs_encoder.transform(data['Profession'].to_numpy().reshape(-1, 1))\n",
    "#Create a Pandas DataFrame of the hot encoded column\n",
    "ohe_df = pd.DataFrame(transformed, columns=jobs_encoder.get_feature_names())\n",
    "#concat with original data\n",
    "data = pd.concat([data, ohe_df], axis=1).drop(['Profession'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One-Hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "\n",
    "enconding_cat = onehot_encoder.fit_transform(features_c.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enconding_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed[name_features_c] = enconding_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OneHotEnconder__: Although LightGBM is capable of using \"true\" categorical variables, to reduce noise while getting the splits for most useful categories I applied OneHotEnconding technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    ">Values of `-1` indicate that the feature was missing from the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing = []\n",
    "\n",
    "for f in train_df.columns:\n",
    "    missings = train_df[train_df[f] == -1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings/train_df.shape[0]\n",
    "        \n",
    "        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
    "        \n",
    "print('\\nIn total, there are {} variables with missing values'.format(len(vars_with_missing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature elimination. I dropped all of calc features and ['ps_ind_14','ps_car_10_cat','ps_car_14','ps_ind_10_bin','ps_ind_11_bin',\n",
    "'ps_ind_12_bin','ps_ind_13_bin','ps_car_11','ps_car_12']. I was excluding them one by one in greedy fashion and checking lgb cross validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Only `ps_car_03_cat` and `ps_car_05_cat` have a large number of missing values.\n",
    "- ps_car_03_cat has 411231 records (69.09%)\n",
    "- ps_car_05_cat has 266551 records (44.78%)\n",
    "\n",
    "__I will drop those variables???__\n",
    "\n",
    "__Add 'number of missing values per data' as a new feature?__\n",
    "train_df['num_missing'] = (all_data==-1).sum(axis=1)\n",
    "\n",
    "\n",
    "Add num_missin to remaining_features\n",
    "remaining_features.append('num_missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the cardinality of the categorical variables - Future Work?\n",
    "\n",
    "### Distinct values - hot enconding\n",
    "Cardinality refers to the number of different values in a variable. As we will create dummy variables from the categorical variables later on, we need to check whether there are variables with many distinct values. We should handle these variables differently as they would result in many dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train_df.copy().replace(-1, np.NaN)\n",
    "train_copy = train_copy.dropna()\n",
    "\n",
    "v = meta[(meta.level == 'interval') & (meta.keep)].index\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cont_corr = train_copy[v].corr() # Correlation between continuous features\n",
    "sns.heatmap(cont_corr, annot=True, cmap='OrRd'); # Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = meta[(meta.level == 'ordinal') & (meta.keep)].index\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "cont_corr = train_copy[v].corr() # Correlation between continuous features\n",
    "sns.heatmap(cont_corr, annot=True); # Plot heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target` variable 1 means that a claim was filed and 0 that it was not claimed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = train_df.target\n",
    "ax = sns.countplot(y,label=\"Count\")\n",
    "\n",
    "total_size = len(train_df)\n",
    "\n",
    "# Display the target value ratio at the top of the bar\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    width = patch.get_width()\n",
    "    left_coord = patch.get_x()\n",
    "    percent = height/total_size*100\n",
    "\n",
    "    ax.text(x=left_coord + width/2.0, \n",
    "            y=height + 3000,\n",
    "            s='{:1.1f}%'.format(percent),\n",
    "            ha='center')\n",
    "\n",
    "ax.set_title('Target Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "__Reference__\n",
    ">Raschka, S., & Mirjalili, V. (2019). Python Machine Learning. Zaltbommel, Netherlands: Van Haren Publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimport warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "        'class_weight': 'balanced', # as we have imbalanced class\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'dart',\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 500,\n",
    "        'n_jobs': 4, # number of parallel threads\n",
    "    }\n",
    "features_classifier = lgb.LGBMClassifier()  \n",
    "features_classifier.set_params(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.target\n",
    "X = train_df.drop(['target'], inplace=False, axis=1)\n",
    "\n",
    "X_test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "predicts = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(\"---\")\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    features_classifier.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    predicts.append(features_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting features importances\n",
    "\n",
    "feature_imp = pd.DataFrame(sorted(zip(features_classifier.feature_importances_,X.columns)), columns=['Value','Feature'])\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features Importance by avg over folds')\n",
    "plt.savefig('./plots/lgbm_importances-01.png', dpi=fig.dpi)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(train_df.drop(['target'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__SelectFromModel__ - Meta-transformer for selecting features based on importance weights.\n",
    "\n",
    "With the get_support method we can then limit the number of variables in the train data.\n",
    "\n",
    ">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "#sfm = SelectFromModel(model, threshold='median', prefit=True)\n",
    "\n",
    "sfm = SelectFromModel(clf, threshold=100, prefit=True)\n",
    "X_selected = sfm.transform(X_train)\n",
    "print('Number of features before selection: {}'.format(X_train.shape[1]))\n",
    "n_features = sfm.transform(X_train).shape[1]\n",
    "print('Number of features after selection: {}'.format(n_features))\n",
    "selected_vars = list(feat_labels[sfm.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[selected_vars + ['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">3- Models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I will use 2 Ensemble ML models: `LightGBM` and `XGBoost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Search for Hyper-Parameter Optimization\n",
    "- Grid search\n",
    "- Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">4- Evaluation</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Gini coefficient\n",
    "\n",
    "$$ \n",
    "Gini = 2 * AUC - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for why use this instead of the commonly used AUC, the only reason I can think of is that a random prediction will yield a Gini score of 0 as opposed to the AUC which will be 0.5.\n",
    "\n",
    "Besides that using the gini coefficient sets the performance of a random classifier to a score of 0... the normalization \"improves\" the other end of the scale and makes that the score of a perfect classifier is equal to 1 rather than a maximum achievable AUC<1. The improvement being only relative depending on whether you think a more intuitive scale is good or not. Although beyond this easier interpretation you might argue that it (the normalization) also improves generalization and comparison of different data-sets.\n",
    "\n",
    "__Reference:__\n",
    "\n",
    "> Hand, D. J., & Till, R. J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171–186. https://doi.org/10.1023/a:1010920819831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import make_scorer, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_normalized(y_actual, y_pred):\n",
    "    \"\"\"Simple normalized Gini based on Scikit-Learn's roc_auc_score\"\"\"\n",
    "    gini = lambda a, p: 2 * roc_auc_score(a, p) - 1\n",
    "    return gini(y_actual, y_pred) / gini(y_actual, y_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out that the culprit was how Scikit-Learn scored the hold-out set. By default, it predicts using the predict method on the model rather than the predict_proba method. The output from predict on a classification problem is the class labels while the output from predict_proba is the probabilities for the class labels. For computing the Gini value on the results, the output of predict_proba is more appropriate.\n",
    "\n",
    "To ensure this happens, we modify the `gini_normalized` function to allow that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_normalized(y_actual, y_pred):\n",
    "    \"\"\"Simple normalized Gini based on Scikit-Learn's roc_auc_score\"\"\"\n",
    "    \n",
    "    # If the predictions y_pred are binary class probabilities\n",
    "    if y_pred.ndim == 2:\n",
    "        if y_pred.shape[1] == 2:\n",
    "            y_pred = y_pred[:, 1]\n",
    "    gini = lambda a, p: 2 * roc_auc_score(a, p) - 1\n",
    "    return gini(y_actual, y_pred) / gini(y_actual, y_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "# <p style=\"background-color:#018cb7; font-size:100%; text-align:left; color:#FFFFFF; padding: 15px 5px 15px 25px; border-radius: 15px;\">5- Kaggle Submission</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
